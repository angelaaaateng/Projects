{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender System - Marketplace Matching\n",
    "\n",
    "In this notebook, we will: \n",
    "- Clean textual data from user-input verbatim posts\n",
    "- Use a Word2Vec model to calculate document similarities \n",
    "- Sort the most similar user input to our training data in order to recommend similar products \n",
    "- Save this model in a format that allows us to refresh the testing data\n",
    "\n",
    "The goal of this project is to create a recommender system to help Pangeans find the right \"project\" for them, given their profile information. Here, we are using legacy data that is from Pangea V2, when Pangeans were allowed to post services and requests, as well as purchase items on the platform. We are using the User-Inputted Titles to suggest similar services, or in V3, similar \"projects\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/angelateng/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Importing Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "from pandas import DataFrame\n",
    "\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import csv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "from gensim import corpora\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.test.utils import common_texts\n",
    "\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "nltk.download('stopwords') ###\n",
    "\n",
    "import gensim \n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "\n",
    "from operator import itemgetter, attrgetter\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "import os, sys\n",
    "\n",
    "from operator import add\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from adjustText import adjust_text\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import pickle\n",
    "\n",
    "import json\n",
    "from pprint import pprint\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "import operator\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def vectorize_and_store_existing_titles():\n",
    "    \n",
    "    raw = pd.read_csv(\"allPostData.csv\", header=0);\n",
    "    \n",
    "    #we can replace this with a filepath in the future\n",
    "    titles = raw['title'];\n",
    "    post_titles = [title for title in titles];\n",
    "    tokens = [[word for word in title.lower().split()] for title in post_titles];\n",
    "    \n",
    "    clean_words = [[word for word in title if word.isalpha()] for title in tokens];\n",
    "    stoplist = set(stopwords.words('english'));\n",
    "    \n",
    "    titles_nostopwords = [[word for word in title if word not in stoplist] for title in clean_words];   \n",
    "    \n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True); \n",
    "    filtered_word_list = [[word for word in title if word in model.vocab] for title in titles_nostopwords];\n",
    "    \n",
    "    title_vectors = {}\n",
    "    for title in filtered_word_list: \n",
    "        word_vecs = [model[word] for word in title]\n",
    "        if len(word_vecs) == 0:\n",
    "            title_vec = [np.zeros(300)]\n",
    "        else: \n",
    "            title_vec = normalize(sum(word_vecs).reshape(1, -1))\n",
    "        title_vectors[\" \".join(title)] = title_vec[0]\n",
    "    #do we want to print out the original titles? how to do this?\n",
    "    vectorized_titles = pd.DataFrame.from_dict(title_vectors)\n",
    "    \n",
    "    #can also replace filepath in the future\n",
    "    vectorized_titles.to_pickle(\"/Users/angelateng/Dropbox/SharpestMinds/vectorized_titles.pkl\")\n",
    "    return(vectorized_titles)\n",
    "\n",
    "#vectorize_and_store_existing_titles()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.read_pickle(\"/Users/angelateng/Dropbox/SharpestMinds/vectorized_titles.pkl\")\n",
    "#sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#can change this directory later\n",
    "\n",
    "with open('allPostData.json') as fresh_data:\n",
    "    user_post = json.load(fresh_data)\n",
    "\n",
    "json_df = pd.DataFrame.from_dict(json_normalize(user_post), orient='columns')\n",
    "\n",
    "def vectorize_new_title(json_df):\n",
    "    title = json_df[\"title\"]\n",
    "    \n",
    "    json_post_titles = [title for title in title];\n",
    "    json_tokens = [[word for word in title.lower().split()] for title in json_post_titles];\n",
    "    \n",
    "    json_clean_words = [[word for word in title if word.isalpha()] for title in json_tokens];\n",
    "    \n",
    "    stoplist = set(stopwords.words('english'));\n",
    "    json_titles_nostopwords = [[word for word in title if word not in stoplist] for title in json_clean_words];   \n",
    "    \n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True); \n",
    "    json_filtered_word_list = [[word for word in title if word in model.vocab] for title in json_titles_nostopwords];\n",
    "    \n",
    "    json_title_vectors = {}\n",
    "    for title in json_filtered_word_list: \n",
    "        json_word_vecs = [model[word] for word in title]\n",
    "        if len(json_word_vecs) == 0:\n",
    "            json_title_vec = [np.zeros(300)]\n",
    "        else: \n",
    "            json_title_vec = normalize(sum(json_word_vecs).reshape(1, -1))\n",
    "        json_title_vectors[\" \".join(title)] = json_title_vec[0]\n",
    "    #do we want to print out the original titles? how to do this?\n",
    "    json_vectorized_titles = pd.DataFrame.from_dict(json_title_vectors)\n",
    "\n",
    "    return(json_vectorized_titles)\n",
    "\n",
    "#vectorize_new_title(json_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_title = vectorize_new_title(json_df)\n",
    "\n",
    "def rank_existing_titles(vectorized_title):\n",
    "    #loop over all keys in dict \n",
    "    ranked_titles = {}\n",
    "    other_titles = pd.read_pickle(\"/Users/angelateng/Dropbox/SharpestMinds/vectorized_titles.pkl\")\n",
    "    #can also use title_vectors.keys() \n",
    "    for key in vectorized_title:\n",
    "        ranked_titles[key] = np.dot(other_titles[key], vectorized_title[key])\n",
    "    sorted_title_vecs = sorted(ranked_titles.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    #print(sorted_title_vecs)\n",
    "    return(sorted_title_vecs)\n",
    "\n",
    "#rank_existing_titles(vectorized_title[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recommendations(json_df):\n",
    "    title = json_df[\"title\"]\n",
    "    vectorized_title = vectorize_new_title(json_df)\n",
    "    ranked_titles = rank_existing_titles(vectorized_title)\n",
    "    \n",
    "    with open(\"/Users/angelateng/Dropbox/SharpestMinds/ranked_titles.csv\", \"w\", newline='') as myfile:\n",
    "        wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "        for title in ranked_titles:\n",
    "            wr.writerow([ranked_titles, title])\n",
    "    #csv_out = ranked_titles.to_csv('ranked_titles', encoding='utf-8', index=False)\n",
    "    #writer = csv.writer(ranked_titles, delimiter='', quotechar='\"', quoting=csv.QUOTE_ALL)\n",
    " \n",
    "    #for row in reader:\n",
    "        #writer.writerow(row)\n",
    "\n",
    "       # with open('csv_out', 'wb') as csv_out:\n",
    "         #   csv_writer = csv.writer(csv_out, quoting=csv.QUOTE_ALL)\n",
    "         #   csv_writer.writerow('csv_out')\n",
    "    return(ranked_titles)\n",
    "    \n",
    "    \n",
    "\n",
    "#vectorize_and_store_existing_titles(); \n",
    "#vectorize_new_title(json_df);\n",
    "#rank_existing_titles(vectorize_new_title);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ride boston', 1.0000004),\n",
      " ('league legends booster', 1.0000004),\n",
      " ('video making lessons', 1.0000004),\n",
      " ('fix bike', 1.0000004),\n",
      " ('tattoo design', 1.0000004),\n",
      " ('web development design', 1.0000004),\n",
      " ('personal shopper', 1.0000004),\n",
      " ('startup opportunity', 1.0000004),\n",
      " ('spiritual talks', 1.0000004),\n",
      " ('fashion designer', 1.0000004)]\n"
     ]
    }
   ],
   "source": [
    "#generate_recommendations(json_df)\n",
    "pprint(generate_recommendations(json_df)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
